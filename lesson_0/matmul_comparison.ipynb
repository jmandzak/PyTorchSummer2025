{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a82e780",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import typing\n",
    "\n",
    "import numpy as np\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "422180a8-ecab-41e6-a916-9c4df84175c1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36e6e338",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check for access to GPU // NVIDIA -- CUDA\n",
    "if torch.cuda.is_available():\n",
    "    gpu_backend = \"cuda\"\n",
    "    def time_torch_gpu_mm(\n",
    "        a: torch.Tensor, b: torch.Tensor\n",
    "    ) -> typing.Tuple[float, torch.Tensor]:\n",
    "        assert a.is_cuda and b.is_cuda, \"Both tensors must be on the GPU\"\n",
    "        start = time.time()\n",
    "        val = torch.mm(a, b)\n",
    "        end = time.time()\n",
    "        return end - start, val\n",
    "# Check for access to GPU // Apple Silicon MPS\n",
    "elif torch.backends.mps.is_available():\n",
    "    gpu_backend = \"mps\"\n",
    "    def time_torch_gpu_mm(\n",
    "        a: torch.Tensor, b: torch.Tensor\n",
    "    ) -> typing.Tuple[float, torch.Tensor]:\n",
    "        assert a.is_mps and b.is_mps, \"Both tensors must be on the GPU\"\n",
    "        start = time.time()\n",
    "        val = torch.mm(a, b)\n",
    "        end = time.time()\n",
    "        return end - start, val \n",
    "else: \n",
    "    print(\"GPU Device Not Availible\")\n",
    "\n",
    "\n",
    "def time_torch_cpu_mm(\n",
    "    a: torch.Tensor, b: torch.Tensor\n",
    ") -> typing.Tuple[float, torch.Tensor]:\n",
    "    assert a.is_cpu and b.is_cpu, \"Both tensors must be on the CPU\"\n",
    "    start = time.time()\n",
    "    val = torch.mm(a, b)\n",
    "    end = time.time()\n",
    "    return end - start, val\n",
    "\n",
    "\n",
    "def time_numpy_mm(a: np.ndarray, b: np.ndarray) -> typing.Tuple[float, np.ndarray]:\n",
    "    start = time.time()\n",
    "    val = np.matmul(a, b)\n",
    "    end = time.time()\n",
    "    return end - start, val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b1ab6b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def ensure_correctness(\n",
    "    torch_gpu: torch.Tensor, torch_cpu: torch.Tensor, numpy: np.ndarray\n",
    ") -> None:\n",
    "    torch_gpu_as_np = torch_gpu.cpu().numpy().astype(np.float32)\n",
    "    torch_cpu_as_np = torch_cpu.numpy().astype(np.float32)\n",
    "\n",
    "    ATOL = 0.01\n",
    "    assert np.allclose(\n",
    "        torch_gpu_as_np, torch_cpu_as_np, atol=ATOL\n",
    "    ), \"Torch GPU and CPU results differ\"\n",
    "    assert np.allclose(\n",
    "        torch_cpu_as_np, numpy, atol=ATOL\n",
    "    ), \"Torch CPU and NumPy results differ\"\n",
    "    assert np.allclose(\n",
    "        torch_gpu_as_np, numpy, atol=ATOL\n",
    "    ), \"Torch GPU and NumPy results differ\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ffb7908",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.manual_seed(0)\n",
    "np.random.seed(0)\n",
    "SIZE = 10000\n",
    "\n",
    "# Create random tensors\n",
    "a = torch.randn(SIZE, SIZE, device=gpu_backend, dtype=torch.float32)\n",
    "b = torch.randn(SIZE, SIZE, device=gpu_backend, dtype=torch.float32)\n",
    "a_cpu = a.cpu()\n",
    "b_cpu = b.cpu()\n",
    "a_numpy = a_cpu.numpy().astype(np.float32)\n",
    "b_numpy = b_cpu.numpy().astype(np.float32)\n",
    "\n",
    "ATOL = 0.0001\n",
    "assert np.allclose(\n",
    "    a.cpu().numpy(), a_cpu.numpy(), atol=ATOL\n",
    "), f\"Torch GPU and CPU results differ: {a.cpu().numpy()[0][0]} vs {a_cpu.numpy()[0][0]}\"\n",
    "assert np.allclose(\n",
    "    a_cpu.numpy(), a_numpy, atol=ATOL\n",
    "), f\"Torch CPU and NumPy results differ: {a_cpu.numpy()[0][0]} vs {a_numpy[0][0]}\"\n",
    "assert np.allclose(\n",
    "    a.cpu().numpy(), a_numpy, atol=ATOL\n",
    "), f\"Torch GPU and NumPy results differ: {a.cpu().numpy()[0][0]} vs {a_numpy[0][0]}\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4fabf58f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Time the operations\n",
    "print(f\"Timing {SIZE:,} x {SIZE:,} matrix multiplication...\")\n",
    "torch_gpu_time, torch_gpu_result = time_torch_gpu_mm(a, b)\n",
    "torch_cpu_time, torch_cpu_result = time_torch_cpu_mm(a_cpu, b_cpu)\n",
    "numpy_time, numpy_result = time_numpy_mm(a_numpy, b_numpy)\n",
    "\n",
    "# Ensure correctness\n",
    "ensure_correctness(torch_gpu_result, torch_cpu_result, numpy_result)\n",
    "\n",
    "# Print the results\n",
    "print(f\"Time taken by Torch GPU: {torch_gpu_time:.6f} seconds\")\n",
    "print(f\"Time taken by Torch CPU: {torch_cpu_time:.6f} seconds\")\n",
    "print(f\"Time taken by NumPy: {numpy_time:.6f} seconds\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
